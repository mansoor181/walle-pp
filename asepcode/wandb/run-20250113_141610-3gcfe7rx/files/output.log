{'callbacks': {'early_stopping': {'metric_name': 'valEpoch/avg_epi_node_mcc',
                                  'min_delta': 0.0,
                                  'minimize': False,
                                  'patience': 10},
               'lr_scheduler': {'kwargs': {'gamma': 0.9, 'step_size': 10},
                                'name': 'StepLR',
                                'step': None},
               'model_checkpoint': {'k': 3,
                                    'metric_name': 'valEpoch/avg_epi_node_mcc',
                                    'minimize': False,
                                    'save_dir': '/home/mansoor/antibody_design/epitope_prediction/epitope_pred/asepcode/../../results/asep/ckpts/20250113-141610'},
               'model_checkpoint_edge': {'k': 3,
                                         'metric_name': 'valEpoch/avg_edge_index_bg_mcc',
                                         'minimize': False,
                                         'save_dir': '/home/mansoor/antibody_design/epitope_prediction/epitope_pred/asepcode/../../results/asep/ckpts/edge/20250113-141610'}},
 'dataset': {'ab': {'custom_embedding_method': None,
                    'custom_embedding_method_src': {'method_name': 'embed_protbert',
                                                    'name': 'ProtBERT',
                                                    'script_path': 'asep/data/embedding/protbert.py'},
                    'embedding_model': 'esm2'},
             'ag': {'custom_embedding_method': None,
                    'custom_embedding_method_src': {'method_name': 'embed_esm2',
                                                    'name': 'ESM2',
                                                    'script_path': 'asep/data/embedding/esm2.py'},
                    'embedding_model': 'esm2'},
             'name': 'asep',
             'node_feat_type': 'custom',
             'root': '/home/mansoor/antibody_design/epitope_prediction/data',
             'split_idx': None,
             'split_method': 'epitope_group'},
 'hparams': {'act_list': [None],
             'batch_size': 128,
             'decoder': {'name': 'inner_prod'},
             'dim_list': [128, 64],
             'edge_cutoff': 0.5,
             'input_ab_act': 'relu',
             'input_ab_dim': 1024,
             'input_ag_act': 'relu',
             'input_ag_dim': 1280,
             'max_epochs': 300,
             'model_type': 'linear',
             'num_edge_cutoff': 3,
             'pos_weight': 100,
             'test_batch_size': 32,
             'train_batch_size': 128,
             'val_batch_size': 32},
 'keep_interim_ckpts': True,
 'logging_method': 'wandb',
 'loss': {'edge_index_bg_rec_loss': {'kwargs': {'reduction': 'mean',
                                                'weight_tensor': 100},
                                     'name': 'edge_index_bg_rec_loss',
                                     'w': 1.0},
          'edge_index_bg_sum_loss': {'kwargs': {'thr': 40},
                                     'name': 'edge_index_bg_sum_loss',
                                     'w': 0.0003942821556421417}},
 'mode': 'train',
 'num_threads': 4,
 'optimizer': {'name': 'Adam', 'params': {'lr': 0.001, 'weight_decay': 0.0}},
 'seed': 588867788,
 'try_gpu': True,
 'wandb_init': {'entity': 'alibilab-gsu',
                'group': 'train',
                'job_type': 'train',
                'notes': 'protbert-esm2',
                'project': 'retrain-walle-group',
                'tags': 'protbert-esm2'},
 'work_dir': '/home/mansoor/antibody_design/epitope_prediction/epitope_pred/asepcode'}
wandb run info:
- name: quiet-capybara-101
- project: retrain-walle-group
- entity: alibilab-gsu
2025-01-13 14:16:11.770 | INFO     | __main__:main:116 - final config:
{'callbacks': {'early_stopping': {'metric_name': 'valEpoch/avg_epi_node_mcc',
                                  'min_delta': 0.0,
                                  'minimize': False,
                                  'patience': 10},
               'lr_scheduler': {'kwargs': {'gamma': 0.9, 'step_size': 10},
                                'name': 'StepLR',
                                'step': None},
               'model_checkpoint': {'k': 3,
                                    'metric_name': 'valEpoch/avg_epi_node_mcc',
                                    'minimize': False,
                                    'save_dir': '/home/mansoor/antibody_design/epitope_prediction/epitope_pred/asepcode/../../results/asep/ckpts/20250113-141610/20250113-141611'},
               'model_checkpoint_edge': {'k': 3,
                                         'metric_name': 'valEpoch/avg_edge_index_bg_mcc',
                                         'minimize': False,
                                         'save_dir': '/home/mansoor/antibody_design/epitope_prediction/epitope_pred/asepcode/../../results/asep/ckpts/edge/20250113-141610/20250113-141611'}},
 'dataset': {'ab': {'custom_embedding_method': None,
                    'custom_embedding_method_src': {'method_name': 'embed_protbert',
                                                    'name': 'ProtBERT',
                                                    'script_path': 'asep/data/embedding/protbert.py'},
                    'embedding_model': 'esm2'},
             'ag': {'custom_embedding_method': None,
                    'custom_embedding_method_src': {'method_name': 'embed_esm2',
                                                    'name': 'ESM2',
                                                    'script_path': 'asep/data/embedding/esm2.py'},
                    'embedding_model': 'esm2'},
             'name': 'asep',
             'node_feat_type': 'custom',
             'root': '/home/mansoor/antibody_design/epitope_prediction/data',
             'split_idx': None,
             'split_method': 'epitope_group'},
 'hparams': {'act_list': [None],
             'batch_size': 128,
             'decoder': {'name': 'inner_prod'},
             'dim_list': [128, 64],
             'edge_cutoff': 0.5,
             'input_ab_act': 'relu',
             'input_ab_dim': 1024,
             'input_ag_act': 'relu',
             'input_ag_dim': 1280,
             'max_epochs': 300,
             'model_type': 'linear',
             'num_edge_cutoff': 3,
             'pos_weight': 100,
             'test_batch_size': 32,
             'train_batch_size': 128,
             'val_batch_size': 32},
 'keep_interim_ckpts': True,
 'logging_method': 'wandb',
 'loss': {'edge_index_bg_rec_loss': {'kwargs': {'reduction': 'mean',
                                                'weight_tensor': 100},
                                     'name': 'edge_index_bg_rec_loss',
                                     'w': 1.0},
          'edge_index_bg_sum_loss': {'kwargs': {'thr': 40},
                                     'name': 'edge_index_bg_sum_loss',
                                     'w': 0.0003942821556421417}},
 'mode': 'train',
 'num_threads': 4,
 'optimizer': {'name': 'Adam', 'params': {'lr': 0.001, 'weight_decay': 0.0}},
 'seed': 588867788,
 'try_gpu': True,
 'wandb_init': {'entity': 'alibilab-gsu',
                'group': 'train',
                'job_type': 'train',
                'notes': 'protbert-esm2',
                'project': 'retrain-walle-group',
                'tags': 'protbert-esm2'},
 'work_dir': '/home/mansoor/antibody_design/epitope_prediction/epitope_pred/asepcode'}
2025-01-13 14:16:11.771 | DEBUG    | asep.train_model:train_model:455 - config:
{'callbacks': {'early_stopping': {'metric_name': 'valEpoch/avg_epi_node_mcc',
                                  'min_delta': 0.0,
                                  'minimize': False,
                                  'patience': 10},
               'lr_scheduler': {'kwargs': {'gamma': 0.9, 'step_size': 10},
                                'name': 'StepLR',
                                'step': None},
               'model_checkpoint': {'k': 3,
                                    'metric_name': 'valEpoch/avg_epi_node_mcc',
                                    'minimize': False,
                                    'save_dir': '/home/mansoor/antibody_design/epitope_prediction/epitope_pred/asepcode/../../results/asep/ckpts/20250113-141610/20250113-141611'},
               'model_checkpoint_edge': {'k': 3,
                                         'metric_name': 'valEpoch/avg_edge_index_bg_mcc',
                                         'minimize': False,
                                         'save_dir': '/home/mansoor/antibody_design/epitope_prediction/epitope_pred/asepcode/../../results/asep/ckpts/edge/20250113-141610/20250113-141611'}},
 'dataset': {'ab': {'custom_embedding_method': None,
                    'custom_embedding_method_src': {'method_name': 'embed_protbert',
                                                    'name': 'ProtBERT',
                                                    'script_path': 'asep/data/embedding/protbert.py'},
                    'embedding_model': 'esm2'},
             'ag': {'custom_embedding_method': None,
                    'custom_embedding_method_src': {'method_name': 'embed_esm2',
                                                    'name': 'ESM2',
                                                    'script_path': 'asep/data/embedding/esm2.py'},
                    'embedding_model': 'esm2'},
             'name': 'asep',
             'node_feat_type': 'custom',
             'root': '/home/mansoor/antibody_design/epitope_prediction/data',
             'split_idx': None,
             'split_method': 'epitope_group'},
 'hparams': {'act_list': [None],
             'batch_size': 128,
             'decoder': {'name': 'inner_prod'},
             'dim_list': [128, 64],
             'edge_cutoff': 0.5,
             'input_ab_act': 'relu',
             'input_ab_dim': 1024,
             'input_ag_act': 'relu',
             'input_ag_dim': 1280,
             'max_epochs': 300,
             'model_type': 'linear',
             'num_edge_cutoff': 3,
             'pos_weight': 100,
             'test_batch_size': 32,
             'train_batch_size': 128,
             'val_batch_size': 32},
 'keep_interim_ckpts': True,
 'logging_method': 'wandb',
 'loss': {'edge_index_bg_rec_loss': {'kwargs': {'reduction': 'mean',
                                                'weight_tensor': 100},
                                     'name': 'edge_index_bg_rec_loss',
                                     'w': 1.0},
          'edge_index_bg_sum_loss': {'kwargs': {'thr': 40},
                                     'name': 'edge_index_bg_sum_loss',
                                     'w': 0.0003942821556421417}},
 'mode': 'train',
 'num_threads': 4,
 'optimizer': {'name': 'Adam', 'params': {'lr': 0.001, 'weight_decay': 0.0}},
 'seed': 588867788,
 'try_gpu': True,
 'wandb_init': {'entity': 'alibilab-gsu',
                'group': 'train',
                'job_type': 'train',
                'notes': 'protbert-esm2',
                'project': 'retrain-walle-group',
                'tags': 'protbert-esm2'},
 'work_dir': '/home/mansoor/antibody_design/epitope_prediction/epitope_pred/asepcode'}
Loading custom embedding function from asep/data/embedding/protbert.py
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loaded function embed_protbert from asep/data/embedding/protbert.py
Output shape from custom embedding function: torch.Size([20, 1024])
Custom embedding function loaded: <function embed_protbert at 0x7f1a686ea050>
Loading custom embedding function from asep/data/embedding/esm2.py
Using device: cpu
Loaded function embed_esm2 from asep/data/embedding/esm2.py
Output shape from custom embedding function: torch.Size([20, 1280])
Custom embedding function loaded: <function embed_esm2 at 0x7f1a686ab9a0>
Reading from custom embedding file: ProtBERT_ESM2_emb.pt
Train batch x_b shape: torch.Size([7613, 1024])
Train batch x_g shape: torch.Size([37859, 1280])
Val batch x_b shape: torch.Size([1952, 1024])
Val batch x_g shape: torch.Size([9187, 1280])
Test batch x_b shape: torch.Size([1954, 1024])
Test batch x_g shape: torch.Size([9265, 1280])
Error executing job with overrides: ['mode=train', 'wandb_init.project=retrain-walle-group', 'wandb_init.entity=alibilab-gsu', 'wandb_init.notes=protbert-esm2', 'wandb_init.tags=protbert-esm2', 'dataset.node_feat_type=custom', 'dataset.ab.custom_embedding_method_src.script_path=asep/data/embedding/protbert.py', 'dataset.ab.custom_embedding_method_src.method_name=embed_protbert', 'dataset.ab.custom_embedding_method_src.name=ProtBERT', 'dataset.ag.custom_embedding_method_src.script_path=asep/data/embedding/esm2.py', 'dataset.ag.custom_embedding_method_src.method_name=embed_esm2', 'dataset.ag.custom_embedding_method_src.name=ESM2', 'hparams.input_ab_dim=1024', 'hparams.input_ag_dim=1280', 'hparams.model_type=linear', 'dataset.split_method=epitope_group', 'hparams.max_epochs=300', 'hparams.pos_weight=100', 'hparams.train_batch_size=128', 'hparams.val_batch_size=32', 'hparams.test_batch_size=32']
Traceback (most recent call last):
  File "/home/mansoor/anaconda3/envs/bio_cpu_venv/lib/python3.10/site-packages/torch/serialization.py", line 850, in save
    _save(
  File "/home/mansoor/anaconda3/envs/bio_cpu_venv/lib/python3.10/site-packages/torch/serialization.py", line 1114, in _save
    zip_file.write_record(name, storage, num_bytes)
OSError: [Errno 28] No space left on device

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/mansoor/antibody_design/epitope_prediction/epitope_pred/asepcode/train.py", line 117, in main
    train_model(
  File "/home/mansoor/antibody_design/epitope_prediction/epitope_pred/asepcode/asep/train_model.py", line 469, in train_model
    train_loader, val_loader, test_loader = create_asepv1_dataloaders(
  File "/home/mansoor/antibody_design/epitope_prediction/epitope_pred/asepcode/asep/train_model.py", line 244, in create_asepv1_dataloaders
    torch.save(train_set[0], f)
  File "/home/mansoor/anaconda3/envs/bio_cpu_venv/lib/python3.10/site-packages/torch/serialization.py", line 849, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/mansoor/anaconda3/envs/bio_cpu_venv/lib/python3.10/site-packages/torch/serialization.py", line 706, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:603] . unexpected pos 427259136 vs 427259088

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
